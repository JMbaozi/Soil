import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import ast  # ç”¨äºå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸


# ===== è®¾ç½®ä¸­æ–‡å­—ä½“ =====
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # æˆ– ['SimHei'] é»‘ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

class SpectrumProcessor:
    # è®¡ç®—å…‰è°±æ•°æ®çš„å¹³å‡ counts å¹¶åˆå¹¶ç»çº¬åº¦ç›¸åŒçš„ç‚¹ è®¡ç®—æ€»è®¡æ•°ï¼Œä¿å­˜ä¸ºæ–° CSV
    # è¯¥ç±»å·²è¢« SpectrumAnalyzer å–ä»£ï¼Œä¿ç•™ä»¥é˜²éœ€è¦ç®€å•å¤„ç†æ—¶ä½¿ç”¨
    def __init__(self, input_file=None, output_file=None):
        self.input_file = input_file
        self.output_file = output_file
        self.df = None
        if input_file:
            print(f"âœ… SpectrumProcessor åˆå§‹åŒ–ï¼š{input_file} â†’ {output_file}")

    # ===== è¯»å– CSV =====
    def load_data(self):
        print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡ä»¶ï¼š{self.input_file}")
        self.df = pd.read_csv(self.input_file)
        print(f"âœ… å·²è¯»å– {len(self.df)} æ¡è®°å½•")

    # ===== å¹³å‡ countsï¼Œå¹¶å››èˆäº”å…¥ =====
    def _avg_counts(self, counts_list):
        arrs = []
        for c in counts_list:
            if isinstance(c, str):
                arrs.append(np.array(eval(c)))
        if not arrs:
            raise ValueError("âš ï¸ counts æ•°æ®ä¸ºç©º")
        lengths = [len(a) for a in arrs]
        if len(set(lengths)) != 1:
            raise ValueError(f"âš ï¸ counts é•¿åº¦ä¸ä¸€è‡´: {lengths}")
        avg = np.mean(arrs, axis=0)
        return np.round(avg).astype(int)

    # ===== æ•°æ®å¤„ç† =====
    def process_data(self):
        print("âš™ï¸ å¼€å§‹å¤„ç†æ•°æ®...")

        # æ‹† location â†’ lat/lonï¼ˆä¿ç•™å››ä½å°æ•°ï¼‰
        def extract_lat_lon(loc):
            coords = eval(loc)['coordinates']
            return round(coords[1], 4), round(coords[0], 4)

        self.df[['lat', 'lon']] = self.df['location'].apply(lambda x: pd.Series(extract_lat_lon(x)))

        # æŒ‰ç»çº¬åº¦åˆ†ç»„
        group_cols = ['lat', 'lon']
        grouped = self.df.groupby(group_cols)

        # åˆå¹¶ counts
        merged_counts = []
        merged_lat = []
        merged_lon = []
        merged_time = []  # âœ… æ–°å¢ï¼šä¿ç•™ collection_time
        for (lat, lon), group in grouped:
            avg = self._avg_counts(group['counts'].tolist())
            merged_counts.append(avg)
            merged_lat.append(lat)
            merged_lon.append(lon)
            merged_time.append(group['collection_time'].iloc[0])  # âœ… åŒç»„ä¿ç•™ç¬¬ä¸€ä¸ªæ—¶é—´

        # æ„å»ºç»“æœ DataFrameï¼Œåªä¿ç•™å››åˆ—
        result = pd.DataFrame({
            'collection_time': merged_time,  # âœ… æ–°å¢
            'lat': merged_lat,
            'lon': merged_lon,
            'counts': merged_counts
        })

        # total åˆ—
        result['total'] = result['counts'].map(np.sum)

        # ä¿å­˜
        self.df = result
        print(f"ğŸ§ª åˆ†ç»„æ•°é‡ï¼ˆå”¯ä¸€ç‚¹æ•°ï¼‰: {len(result)}")
        print(f"ğŸ§© ç¤ºä¾‹ counts: {result['counts'].iloc[0][:60]}")
        print("âœ… å¤„ç†å®Œæˆ")

    # ===== ä¿å­˜ CSVï¼ˆcounts ä¿å­˜ä¸º list å­—ç¬¦ä¸²ï¼‰ =====
    def save_data(self):
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        df_to_save = self.df.copy()
        df_to_save['counts'] = df_to_save['counts'].apply(lambda x: str(x.tolist()))
        df_to_save.to_csv(self.output_file, index=False)
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{self.output_file}")

    # ===== å•æ–‡ä»¶å®Œæ•´æµç¨‹ =====
    def process(self):
        self.load_data()
        self.process_data()
        self.save_data()


class SpectrumPeak:
    """
    æ ¹æ®æŒ‡å®šé€šé“å’Œçª—å£èŒƒå›´è®¡ç®—èƒ½è°±å³°å¼ºåº¦ï¼ˆçª—å£æ±‚å’Œï¼‰ï¼Œæ”¯æŒæ‰¹é‡å¤šé€šé“å¤„ç†
    è¯¥ç±»å·²è¢« SpectrumAnalyzer å–ä»£ï¼Œä¿ç•™ä»¥é˜²éœ€è¦ç®€å•å¤„ç†æ—¶ä½¿ç”¨
    """
    def __init__(self, counts_column='counts'):
        self.counts_column = counts_column  # CSV ä¸­çš„è®¡æ•°åˆ—å

    def compute_peak(self, counts, channel, window):
        """
        è®¡ç®—æŒ‡å®šé€šé“é™„è¿‘çª—å£çš„è®¡æ•°å’Œ
        :param counts: æ ·æœ¬çš„è®¡æ•°åˆ—è¡¨
        :param channel: ä¸­å¿ƒé€šé“
        :param window: çª—å£èŒƒå›´ï¼ˆæ­£è´ŸèŒƒå›´ï¼‰
        :return: çª—å£å†…è®¡æ•°å’Œ
        """
        start = max(0, channel - window)
        end = min(len(counts), channel + window + 1)  # python åˆ‡ç‰‡æ˜¯å·¦é—­å³å¼€
        return sum(counts[start:end])

    def process_csv(self, input_csv, output_csv, peaks):
        """
        å¤„ç† CSV æ–‡ä»¶ï¼Œè®¡ç®—å³°å¼ºåº¦ï¼Œå¹¶ä¿å­˜æ–° CSV
        :param input_csv: è¾“å…¥ CSV è·¯å¾„
        :param output_csv: è¾“å‡º CSV è·¯å¾„
        :param peaks: dict, {åˆ—å: (ä¸­å¿ƒé€šé“, çª—å£èŒƒå›´)}
                      ä¾‹å¦‚: {'K40_peak': (490, 5), 'U238_peak': (600, 7)}
        """
        df = pd.read_csv(input_csv)

        # å…ˆæŠŠåŸ counts åˆ—è§£ææˆåˆ—è¡¨ï¼Œæ–¹ä¾¿é‡å¤ä½¿ç”¨
        counts_list = df[self.counts_column].apply(ast.literal_eval)

        for peak_name, (channel, window) in peaks.items():
            df[peak_name] = counts_list.apply(lambda x: self.compute_peak(x, channel, window))

        df.to_csv(output_csv, index=False)
        print(f'å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜åˆ° {output_csv}')


import pandas as pd
import numpy as np
import os
import ast

class SpectrumAnalyzer:
    """
    å…‰è°±æ•°æ®åˆ†æç±»ï¼šæ”¯æŒåˆå¹¶å¹³å‡ countsã€è®¡ç®—æ€»è®¡æ•°ä¸å¤šä¸ªå³°å€¼ï¼Œ
    å¹¶å¯ä¿ç•™æŒ‡å®šçš„åŸå§‹åˆ—ï¼ˆå¦‚ speedã€heightï¼‰ã€‚
    """

    def __init__(self, input_file, output_file, peaks=None, extra_columns=None):
        """
        :param input_file: è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„
        :param output_file: è¾“å‡º CSV æ–‡ä»¶è·¯å¾„
        :param peaks: dictï¼Œå½¢å¦‚ {'K40_peak': (490, 20), 'Bi214_peak': (200, 20)}
        :param extra_columns: listï¼Œè¦ä¿ç•™çš„åŸå§‹åˆ—åï¼Œå¦‚ ['speed', 'height']
        """
        self.input_file = input_file
        self.output_file = output_file
        self.peaks = peaks or {}
        self.extra_columns = extra_columns or []
        self.df = None
        print(f"âœ… åˆå§‹åŒ–ï¼š{input_file} â†’ {output_file}")

    # ===== è¯»å– CSV =====
    def load_data(self):
        print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡ä»¶ï¼š{self.input_file}")
        self.df = pd.read_csv(self.input_file)
        print(f"âœ… å·²è¯»å– {len(self.df)} æ¡è®°å½•")

    # ===== å¹³å‡ counts =====
    def _avg_counts(self, counts_list):
        arrs = []
        for c in counts_list:
            if isinstance(c, str):
                arrs.append(np.array(ast.literal_eval(c)))
        if not arrs:
            raise ValueError("âš ï¸ counts æ•°æ®ä¸ºç©º")
        lengths = [len(a) for a in arrs]
        if len(set(lengths)) != 1:
            raise ValueError(f"âš ï¸ counts é•¿åº¦ä¸ä¸€è‡´: {lengths}")
        avg = np.mean(arrs, axis=0)
        return np.round(avg).astype(int)

    # ===== ç»çº¬åº¦æå– =====
    def _extract_lat_lon(self, loc):
        coords = eval(loc)['coordinates']
        return round(coords[1], 6), round(coords[0], 6)

    # ===== å³°å€¼æ±‚å’Œå‡½æ•° =====
    def _compute_peak(self, counts, channel, window):
        start = max(0, channel - window)
        end = min(len(counts), channel + window + 1)
        return int(np.sum(counts[start:end]))

    # ===== æ•°æ®å¤„ç† =====
    def process_data(self):
        print("âš™ï¸ å¼€å§‹å¤„ç†æ•°æ®...")

        # æå–ç»çº¬åº¦
        self.df[['lat', 'lon']] = self.df['location'].apply(lambda x: pd.Series(self._extract_lat_lon(x)))

        # åˆ†ç»„å­—æ®µ
        group_fields = ['lat', 'lon']
        grouped = self.df.groupby(group_fields)

        merged_records = []

        for (lat, lon), group in grouped:
            record = {
                'lat': lat,
                'lon': lon,
                'collection_time': group['collection_time'].iloc[0],
                'counts': self._avg_counts(group['counts'].tolist())
            }

            # é¢å¤–å­—æ®µï¼ˆå¹³å‡æˆ–é¦–å€¼ï¼‰
            for col in self.extra_columns:
                if col in group.columns:
                    # å¦‚æœæ˜¯æ•°å€¼å‹åˆ™æ±‚å¹³å‡ï¼Œå¦åˆ™å–ç¬¬ä¸€ä¸ª
                    if np.issubdtype(group[col].dtype, np.number):
                        record[col] = group[col].mean()
                    else:
                        record[col] = group[col].iloc[0]
                else:
                    record[col] = np.nan  # ç¼ºå¤±å­—æ®µå¡«å……ç©ºå€¼

            merged_records.append(record)

        result = pd.DataFrame(merged_records)

        # ===== è®¡ç®—æ€»è®¡æ•° =====
        result['total'] = result['counts'].map(np.sum)

        # ===== è®¡ç®—å¤šä¸ªå³°å€¼ =====
        for peak_name, (channel, window) in self.peaks.items():
            result[peak_name] = result['counts'].map(lambda c: self._compute_peak(c, channel, window))
            print(f"ğŸ“ˆ å·²è®¡ç®—å³°å€¼åˆ—ï¼š{peak_name}ï¼ˆä¸­å¿ƒ {channel} Â± {window}ï¼‰")

        self.df = result
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(result)} ä¸ªå”¯ä¸€ç‚¹")

    # ===== ä¿å­˜ =====
    def save_data(self):
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        df_to_save = self.df.copy()
        df_to_save['counts'] = df_to_save['counts'].apply(lambda x: str(x.tolist()))
        df_to_save.to_csv(self.output_file, index=False)
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{self.output_file}")

    # ===== ä¸€é”®æ‰§è¡Œå®Œæ•´æµç¨‹ =====
    def run(self):
        self.load_data()
        self.process_data()
        self.save_data()


class SpectrumAnalyzerBatch:
    """
    æ‰¹é‡å…‰è°±åˆ†æå™¨ï¼šæ”¯æŒæ‰‹åŠ¨æŒ‡å®šå¤šä¸ªæ–‡ä»¶è¾“å…¥è¾“å‡ºè·¯å¾„
    """

    def __init__(self, file_pairs, peaks, extra_columns=None):
        """
        :param file_pairs: listï¼Œæ¯ä¸ªå…ƒç´ ä¸º (input_file, output_file)
        :param peaks: dictï¼Œå³°å€¼å®šä¹‰ï¼Œå¦‚ {'K40_peak': (490, 20), 'Bi214_peak': (200, 20)}
        :param extra_columns: listï¼Œè¦ä¿ç•™çš„åŸå§‹åˆ—ï¼Œå¦‚ ['speed', 'height']
        """
        self.file_pairs = file_pairs
        self.peaks = peaks
        self.extra_columns = extra_columns or []

    def run_all(self):
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†...")
        success, failed = 0, 0

        for i, (input_file, output_file) in enumerate(self.file_pairs, start=1):
            print(f"\nğŸ“„ [{i}/{len(self.file_pairs)}] å¤„ç†æ–‡ä»¶ï¼š{os.path.basename(input_file)}")
            try:
                analyzer = SpectrumAnalyzer(
                    input_file, output_file,
                    peaks=self.peaks,
                    extra_columns=self.extra_columns
                )
                analyzer.run()
                success += 1
            except Exception as e:
                failed += 1
                print(f"âŒ æ–‡ä»¶ {input_file} å¤„ç†å¤±è´¥ï¼š{e}")

        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸ {success} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ªã€‚")


class SoilDataExtractor:
    """
    åœŸå£¤å®æµ‹æ•°æ®æå–å™¨
    æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ª CSV æ–‡ä»¶ï¼Œåªä¿ç•™æŒ‡å®šåˆ—å¹¶è§£æç»çº¬åº¦ã€‚
    """

    def __init__(self, file_pairs):
        """
        :param file_pairs: listï¼Œæ¯ä¸ªå…ƒç´ ä¸º (input_file, output_file)
        """
        self.file_pairs = file_pairs

    def extract_fields(self, input_file, output_file):
        """æå–å•ä¸ªæ–‡ä»¶çš„å…³é”®å­—æ®µå¹¶ä¿å­˜"""
        print(f"ğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{input_file}")

        df = pd.read_csv(input_file)

        # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
        required_cols = ['depth_cm', 'sample_time', 'location', 'pH', 'AP', 'NHâ‚„âº-N', 'AK']
        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            raise ValueError(f"âŒ æ–‡ä»¶ {input_file} ç¼ºå°‘åˆ—ï¼š{missing}")

        # è§£æ location -> lat/lon
        def parse_location(loc_str):
            try:
                loc = ast.literal_eval(loc_str)
                coords = loc.get('coordinates', [None, None])
                return pd.Series({'lon': coords[0], 'lat': coords[1]})
            except Exception:
                return pd.Series({'lon': None, 'lat': None})

        df[['lon', 'lat']] = df['location'].apply(parse_location)

        # é€‰å–éœ€è¦çš„åˆ—
        df_out = df[['depth_cm', 'sample_time', 'lat', 'lon', 'pH', 'AP', 'NHâ‚„âº-N', 'AK']].copy()

        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df_out.to_csv(output_file, index=False)
        print(f"âœ… å·²ä¿å­˜åˆ°ï¼š{output_file}ï¼ˆå…± {len(df_out)} æ¡è®°å½•ï¼‰")

    def run_all(self):
        """æ‰¹é‡è¿è¡Œ"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡æå–åœŸå£¤å®æµ‹æ•°æ®...")
        success, failed = 0, 0

        for i, (input_file, output_file) in enumerate(self.file_pairs, start=1):
            print(f"\n[{i}/{len(self.file_pairs)}] å¤„ç† {os.path.basename(input_file)}")
            try:
                self.extract_fields(input_file, output_file)
                success += 1
            except Exception as e:
                print(f"âŒ å‡ºé”™ï¼š{e}")
                failed += 1

        print(f"\nğŸ æ‰¹é‡å¤„ç†å®Œæˆï¼šæˆåŠŸ {success} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ªã€‚")


class SpeedTotalAnalyzer:
    def __init__(self, input_file, output_file):
        """
        å•æ–‡ä»¶åˆ†æå™¨
        :param input_file: è¾“å…¥ CSVï¼Œéœ€åŒ…å« 'speed' å’Œ 'total' åˆ—
        :param output_file: è¾“å‡ºå›¾åƒè·¯å¾„ï¼ˆå«æ–‡ä»¶åï¼‰
        """
        self.input_file = input_file
        self.output_file = output_file

    def load_data(self):
        """è¯»å–æ•°æ®"""
        df = pd.read_csv(self.input_file)
        if 'speed' not in df.columns or 'total' not in df.columns:
            raise ValueError(f"âš ï¸ æ–‡ä»¶ {self.input_file} ç¼ºå°‘ 'speed' æˆ– 'total' åˆ—")
        self.df = df.sort_values(by='speed')
        print(f"âœ… å·²åŠ è½½ {os.path.basename(self.input_file)}ï¼Œå…± {len(df)} æ¡è®°å½•")

    def plot_total_scatter_with_means(self):
        """ç»˜åˆ¶ä¸åŒé€Ÿåº¦ä¸‹ total çš„æ•£ç‚¹å›¾ + å¹³å‡å€¼çº¿"""
        grouped = self.df.groupby('speed')

        plt.figure(figsize=(9, 6))
        for speed, group in grouped:
            plt.scatter(
                [speed] * len(group),
                group['total'],
                alpha=0.6,
                label=f'Speed={speed}'
            )

        # å¹³å‡å€¼æŠ˜çº¿
        means = grouped['total'].mean().sort_index()
        plt.plot(means.index, means.values, color='black', linewidth=2.5, marker='o', label='Mean Total')

        plt.title(f'Total Counts by Speed\n({os.path.basename(self.input_file)})')
        plt.xlabel('Speed')
        plt.ylabel('Total Counts')
        plt.grid(alpha=0.3, linestyle='--')
        plt.legend(title='Speed', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()

        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        plt.savefig(self.output_file, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ğŸ’¾ å·²ä¿å­˜å›¾åƒè‡³ï¼š{self.output_file}")

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        self.load_data()
        self.plot_total_scatter_with_means()


class SpeedTotalBatchAnalyzer:
    def __init__(self, file_pairs):
        """
        æ‰¹é‡åˆ†æå™¨
        :param file_pairs: åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(input_file, output_file), ...]
        """
        self.file_pairs = file_pairs

    def run_all(self):
        print("ğŸš€ å¼€å§‹æ‰¹é‡ç»˜åˆ¶é€Ÿåº¦-Totalå…³ç³»å›¾...")
        success, failed = 0, 0

        for i, (input_file, output_file) in enumerate(self.file_pairs, start=1):
            print(f"\nğŸ“„ [{i}/{len(self.file_pairs)}] æ­£åœ¨å¤„ç†ï¼š{os.path.basename(input_file)}")
            try:
                analyzer = SpeedTotalAnalyzer(input_file, output_file)
                analyzer.run()
                success += 1
            except Exception as e:
                failed += 1
                print(f"âŒ æ–‡ä»¶ {input_file} å¤„ç†å¤±è´¥ï¼š{e}")

        print(f"\nâœ… æ‰¹é‡ç»˜åˆ¶å®Œæˆï¼æˆåŠŸ {success} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ªã€‚")